{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install tqdm\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # 서버와 통신할 때 중간중간 시간 지연. 보통은 1초\n",
    "from tqdm import tqdm # for문 돌릴 때 진행상황을 %게이지로 알려준다.\n",
    "from selenium import webdriver\n",
    "import openpyxl\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# 에러\n",
    "from selenium.common.exceptions import NoSuchElementException # element 오류처리\n",
    "\n",
    "# 워닝 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 액셀에서 검색할 데이터들 불러오기\n",
    "load_datas = openpyxl.load_workbook(\"data1.xlsx\")\n",
    "datas = load_datas.active\n",
    "column_A_datas = datas['A'][301:401] # 파일이름\n",
    "column_B_datas = datas['B'][301:401] # 검색 키워드\n",
    "\n",
    "crawling_names = []\n",
    "for i in range(1, len(column_B_datas)):\n",
    "    crawling_names.append((column_A_datas[i].value, column_B_datas[i].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12323538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "# ex) 신라면 블랙, 신라면을 고려안한 크롤링\n",
    "\n",
    "# 네이버 api 사용할 때 필요한 값\n",
    "client_id = \"84DIX0pyMLLkcG2uuHia\"\n",
    "client_secret = \"CRf5UovTxF\"\n",
    "\n",
    "for file_name, crawling_name in crawling_names:\n",
    "    query_txt = '\"' + crawling_name + '\"' # 검색할 값을 따옴표 안에 넣어서 검색해주세요 (ex: \"신라면 블랙\") ( 따옴표 안에 넣고 검색시 이 값이 무조건 존재한 블로그들만 나타내는 걸로 알고 있어요!?)\n",
    "    encText = urllib.parse.quote(query_txt) # 네이버 검색창 검색 키워드 \n",
    "\n",
    "    blog_links = []\n",
    "\n",
    "    for start in range(1, 200, 100): # 블로그 200개 기준으로 작성했습니다\n",
    "        stringStart = str(start) # string형태여야 합니다\n",
    "        url = \"https://openapi.naver.com/v1/search/blog?query=\" + encText +\"&start=\"+stringStart+\"&display=100\" # 네이버 검색 api, start: 시작, display: 보여줄 갯수(100이 최대)\n",
    "\n",
    "        # start(블로그 몇 번째인지?, max값 1000)와 display(한번 검색에 몇 개의 결과를 띄울건지, max값 100)조절해주기\n",
    "        # start는 1, 101, 201 이렇게 시작해주세요\n",
    "        # url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "        if(rescode==200):\n",
    "            response_body = response.read()\n",
    "            #print(response_body.decode('utf-8'))\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "\n",
    "        body = response_body.decode('utf-8')\n",
    "        body = body.replace('\"','')\n",
    "\n",
    "        #블로그 링크들 추출\n",
    "        links = re.findall('link: (.*?),\\ndescription',body)\n",
    "        print('추출 링크 수: ',len(links),'개')\n",
    "\n",
    "        for link in links:\n",
    "            link_a = link.replace('\\\\','') # 링크 안의 '\\\\' 제거\n",
    "            link_b = link_a.replace('?Redirect=Log&logNo=','/') # 링크안에 ?Redirect=Log&logNo= 제거\n",
    "            blog_links.append(link_b) # 현재 링크를 배열에 추가 밑에 크롤링에서 사용\n",
    "\n",
    "    print('총 링크 갯수:',len(blog_links),'개')\n",
    "\n",
    "    #본문 크롤링\n",
    "    path = \"chromedriver.exe\" # 크롬 버전에 맞는 크롬 드라이버를 실행하는 파일 위치랑 똑같은 곳에 놔둬주는게 편해요\n",
    "    driver = webdriver.Chrome(path)\n",
    "\n",
    "    # 블로그 링크를 하나씩 불러와서 크롤링\n",
    "    contents = ''\n",
    "    for i in tqdm(blog_links, desc=file_name): \n",
    "        driver.get(i) #불러온 링크에 접속\n",
    "        driver.implicitly_wait(2) # 엔진 자체에서 로딩(파싱?)되는 시간을 기다려주는 것...?\n",
    "        # time.sleep(1) # driver.implicitly_wait만 써도 괜찮은지 모르겠어서 time.sleep도 넣어놓은 거예요..\n",
    "\n",
    "        print(i+' 링크하고 있습니다')\n",
    "        try: # 네이버 블로그\n",
    "            driver.switch_to.frame(\"mainFrame\")  # 1)iframe접근 \n",
    "            blog_contents = driver.find_element_by_css_selector('div.se-main-container').text\n",
    "            contents += blog_contents + '\\n==========================================================================================\\n'\n",
    "        except NoSuchElementException:\n",
    "            blog_contents = driver.find_element_by_css_selector('div#content-area').text\n",
    "            contents += blog_contents + '\\n==========================================================================================\\n'\n",
    "        except NoSuchElementException: \n",
    "            blog_contents = driver.find_element_by_css_selector('div.contents_style').text # 티스토리인 걸로 알고 있어요 (제가 한건데 기억이 안나요 ㅈㅅㅈㅅ;;;)\n",
    "            contents += blog_contents + '\\n==========================================================================================\\n'\n",
    "        except: # 그 외\n",
    "            pass\n",
    "\n",
    "    # 링크 내용을 txt 파일로 저장 \n",
    "    text_content = open(file_name+\".txt\", 'w', encoding='utf-8') # 파일이름 + 확장자명 : txt\n",
    "    text_content.write(contents)  \n",
    "    text_content.close()\n",
    "    print(query_txt+\" 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
